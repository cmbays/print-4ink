# Pipeline Architecture Implementation — Execution Manifest
# Issue: #192
# Research: docs/research/2026-02-15-pipeline-architecture-research.md
# Plan: docs/tools/work-orchestrator/20260215-pipeline-arch/impl-plan.md
# Merge strategy: Sequential PRs to main (bootstrapping build)

vertical: pipeline-arch

waves:
  # ── Wave 0: Config + Entity Foundation ──────────────────────────────
  - name: "Config + Entity Foundation"
    serial: true
    sessions:
      - topic: pipeline-config-foundation
        stage: build
        prompt: |
          You are implementing Wave 0 of the Pipeline Architecture redesign (#192).

          ## Context
          Read these docs FIRST:
          1. `docs/research/2026-02-15-pipeline-architecture-research.md` — 14 design decisions
          2. `docs/tools/work-orchestrator/20260215-pipeline-arch/impl-plan.md` — full plan (Wave 0 section)
          3. Current config files: `config/stages.json`, `config/workflows.json`, `config/verticals.json`, `config/products.json`, `config/tools.json`
          4. `knowledge-base/src/content.config.ts` — current KB schema
          5. `knowledge-base/src/lib/utils.ts` — label functions
          6. `scripts/work.sh` — current CLI tool

          ## Your Tasks

          ### 1. Config File Migration
          - Create `config/pipeline-types.json` (renamed from workflows.json, with updated stage arrays using NEW short slugs: shape, breadboard, plan). Add description field per type. DELETE `config/workflows.json`.
          - Update `config/stages.json`: rename slugs (shaping->shape, breadboarding->breadboard, implementation-planning->plan), remove stale entries (polish, learnings), remove workAlias fields, keep cooldown with `"pipeline": false`.
          - DELETE `config/verticals.json`.
          - Create `config/pipeline-gates.json` with stage gate definitions: required artifacts per stage, gate types (artifact-exists, human-confirms, human-approves-manifest), auto-mode overrides.

          ### 2. KB Schema — Backward-Compatible Transition
          - In `content.config.ts`:
            - Remove `verticalsConfig` and `workflowsConfig` imports
            - Import `pipelineTypesConfig` from `config/pipeline-types.json`
            - Change `pipeline: z.enum(verticals)` to `pipelineName: z.string()` with `z.preprocess()` that accepts old `pipeline` key too
            - Add `pipelineId: z.string().optional()`
            - Change `pipelineType` enum to use `pipelineTypesConfig`
            - Change `stage: z.enum(stages)` to `z.string()` TEMPORARILY (accepts both old and new slugs)
            - Update strategy collection: `pipelinesCompleted`/`pipelinesLaunched` from enum to `z.array(z.string())`
          - In `utils.ts`:
            - Remove `verticalsConfig` import
            - Update `pipelineLabel()` to use `labelFromSlug()` fallback only
            - Ensure `stageLabelMap` handles both old and new slugs
          - VERIFY: `npm run kb:build` passes with ALL existing frontmatter files unchanged.

          ### 3. Pipeline Entity + Registry
          - Create `scripts/lib/pipeline-entity.sh`: entity CRUD functions (_pipeline_create, _pipeline_read, _pipeline_update, _pipeline_transition), state machine validation (ready->active->building->reviewing->wrapped->cooled), type validation against pipeline-types.json, artifact directory creation (docs/{products|tools}/{slug}/{pipeline-id}/).
          - Create `scripts/lib/pipeline-registry.sh`: JSON registry management at `~/Github/print-4ink-worktrees/.pipeline-registry.json` (_registry_pipeline_init, _add, _get, _list, _update).
          - Source both from work.sh.

          ### 4. Work.sh Consumer Updates
          - Remove verticals.json dependency from `_work_phase()` — accept any kebab-case topic without enum validation.
          - Update `_work_build()` to read `baseBranch` from manifest: `yq -r '.baseBranch // "main"' "$MANIFEST"` and use as branch base for worktree creation.
          - Source new lib files.
          - VERIFY: existing `work` commands still function (work list, work build, work clean).

          ## Validation
          - `npm run kb:build` passes (backward-compat schema accepts old frontmatter)
          - `source scripts/work.sh && work help` — no errors
          - All config files valid JSON
          - Pipeline registry functions work: source scripts, call _registry_pipeline_init, _pipeline_create, _pipeline_read

          ## Output
          Commit message prefix: "feat(pipeline): Wave 0 — config + entity foundation"

  # ── Wave 1: Parallel Migrations ─────────────────────────────────────
  - name: "Parallel Migrations"
    serial: false
    sessions:
      - topic: kb-pipeline-schema
        stage: build
        dependsOn: pipeline-config-foundation
        prompt: |
          You are implementing Wave 1, Task 1.1 of the Pipeline Architecture redesign (#192).

          ## Context
          Read these docs FIRST:
          1. `docs/tools/work-orchestrator/20260215-pipeline-arch/impl-plan.md` — Task 1.1 section
          2. `docs/research/2026-02-15-pipeline-architecture-research.md` — KB Frontmatter Evolution section
          3. `knowledge-base/src/content.config.ts` — current schema (Wave 0 added backward-compat)
          4. `knowledge-base/src/pages/pipelines/[pipeline].astro` — current pipeline page
          5. `knowledge-base/src/components/Sidebar.astro` and `VerticalHealth.astro`

          ## Your Tasks

          ### 1. Frontmatter Migration (~60 files)
          In all files under `knowledge-base/src/content/pipelines/`:
          - Rename `pipeline: <value>` to `pipelineName: <value>`
          - Update stage slugs: shaping->shape, breadboarding->breadboard, implementation-planning->plan, learnings->wrap-up
          - Polish stage references: map to appropriate actual stage (usually build or review)
          Also check `knowledge-base/src/content/sessions/` and strategy docs for similar fields.

          ### 2. Schema Tightening (content.config.ts)
          After all frontmatter is migrated:
          - Remove the backward-compat `z.preprocess()` for pipeline->pipelineName
          - Restore `stage: z.enum(stages)` with the NEW stage slugs from updated stages.json
          - Verify all validation is strict again

          ### 3. Page Updates
          - `[pipeline].astro`: Replace getStaticPaths() — derive routes from unique pipelineName values via getCollection('pipelines') instead of verticalsConfig. Remove verticalsConfig import. Update doc filtering from `s.data.pipeline` to `s.data.pipelineName`.
          - `[pipeline]/[stage].astro`: Similar route derivation from content data.
          - `index.astro`: Update pipeline filter dropdown to populate from unique pipelineName values in content. Remove verticalsConfig import.
          - Update coreStages: ['research', 'breadboard', 'build'] (was 'breadboarding').

          ### 4. Component Updates
          - `Sidebar.astro`: Remove verticalsConfig import, derive pipeline nav items from content collection.
          - `VerticalHealth.astro`: Update stage references, consider renaming display text from "vertical" to "pipeline".

          ## Validation
          - `npm run kb:build` — all docs pass strict validation
          - `npm run kb:dev` — browse pipeline pages, verify routes, filters, stage stepper
          - No remaining references to `config/verticals.json` in knowledge-base/
          - No remaining `pipeline:` (without Name suffix) in frontmatter files

          ## Output
          Commit message prefix: "feat(kb): migrate pipeline schema + frontmatter to new model"

      - topic: work-pipeline-mgmt
        stage: build
        dependsOn: pipeline-config-foundation
        prompt: |
          You are implementing Wave 1, Task 1.2 of the Pipeline Architecture redesign (#192).

          ## Context
          Read these docs FIRST:
          1. `docs/tools/work-orchestrator/20260215-pipeline-arch/impl-plan.md` — Task 1.2 section
          2. `docs/research/2026-02-15-pipeline-architecture-research.md` — Work Command Set section, Pipeline Entity Design section
          3. `scripts/work.sh` — current CLI tool
          4. `scripts/lib/pipeline-entity.sh` — entity functions from Wave 0
          5. `scripts/lib/pipeline-registry.sh` — registry functions from Wave 0
          6. `config/pipeline-types.json` — pipeline type definitions from Wave 0

          ## Your Tasks

          ### 1. Implement `work define` (scripts/lib/pipeline-define.sh)
          Command: `work define <name> [--type <type>] [--issue <number>] [--prompt "<text>"] [--auto]`
          - Generate pipeline ID: YYYYMMDD-<name> (kebab-case enforced)
          - Validate type against config/pipeline-types.json (default: vertical)
          - Create pipeline entity in registry with state: ready
          - If --issue: link to GitHub issue (verify exists with gh issue view)
          - If --prompt and no --issue: create GitHub issue automatically with gh issue create
          - Create artifact directory: docs/{products|tools}/{entity-slug}/{pipeline-id}/
            - For now, default to docs/tools/work-orchestrator/{pipeline-id}/ if no entity specified
            - Products/tools are updated later during pipeline stages
          - Display: pipeline ID, type, state, linked issue, artifact path

          ### 2. Implement `work status` (scripts/lib/pipeline-status.sh)
          - No args (dashboard): group pipelines by state, show ID/name/type/stage/state, progress bars
          - With ID (deep dive): full entity detail, completed stages with artifacts, worktrees, PRs, KB docs
          - Use column formatting for readability

          ### 3. Update `work list`
          - Group infrastructure (worktrees, Zellij sessions, ports) by pipeline ID where possible
          - Fall back to current display for non-pipeline worktrees

          ### 4. Update `work clean`
          - Accept pipeline ID in addition to topic
          - If pipeline ID given: clean all sessions within that pipeline
          - Preserve current topic-based cleanup as fallback

          ### 5. Dispatcher + Help
          - Add `define` and `status` routes to work.sh dispatcher
          - Update help text with new commands
          - Source new lib files

          ## Shell Coding Standards
          - Use local variables, avoid global state
          - Use jq for JSON manipulation (already a dependency via yq)
          - Error messages to stderr
          - Return codes: 0 success, 1 user error, 2 system error
          - Functions prefixed with _pipeline_ or _work_ per convention

          ## Validation
          - `source scripts/work.sh && work help` — shows new commands
          - `work define test-pipe --type bug-fix` — creates entity, displays info
          - `work status` — shows dashboard with test pipeline
          - `work status 20260216-test-pipe` — shows deep dive
          - `work clean 20260216-test-pipe` — cleans up
          - Existing commands (work list, work <topic>) still function

          ## Output
          Commit message prefix: "feat(work): pipeline management commands — define, status, list"

      - topic: skills-stage-update
        stage: build
        dependsOn: pipeline-config-foundation
        prompt: |
          You are implementing Wave 1, Task 1.3 of the Pipeline Architecture redesign (#192).

          ## Context
          Read these docs FIRST:
          1. `docs/tools/work-orchestrator/20260215-pipeline-arch/impl-plan.md` — Task 1.3 section
          2. `docs/research/2026-02-15-pipeline-architecture-research.md` — Canonical Stage Slugs section
          3. `config/stages.json` — updated stage slugs from Wave 0

          ## Key Principle
          Skill NAMES don't change. Only stage SLUG references within skill content change:
          - "shaping" as a stage -> "shape" (but the shaping SKILL is still called "shaping")
          - "breadboarding" as a stage -> "breadboard" (but the breadboarding SKILL is still called "breadboarding")
          - "implementation-planning" as a stage -> "plan"
          - "learnings" -> "wrap-up"
          - "polish" -> clarify as pipeline TYPE, not a stage

          ## Your Tasks

          ### 1. Audit All Skills
          Search all files under `.claude/skills/` for references to old stage names used as stage identifiers.
          Use Grep to find: shaping, breadboarding, implementation-planning, learnings, polish
          Distinguish between: skill name references (keep) vs stage slug references (update).

          ### 2. Update Stage References
          For each skill file found in the audit:
          - Replace stage slug references with new short names
          - Update artifact output path references where they mention stage-based directories
          - Be careful NOT to rename the skill itself (e.g., "Run the shaping skill" stays, but "stage: shaping" becomes "stage: shape")

          ### 3. Update Artifact Path Patterns
          Old patterns: `docs/breadboards/{vertical}-breadboard.md`, `docs/shaping/{topic}/frame.md`
          New patterns: `docs/{products|tools}/{entity-slug}/{pipeline-id}/breadboard.md`
          Note: Skills should document both patterns — entity-first for pipeline-managed work, legacy paths for standalone usage. Skills receive path injection from the pipeline orchestrator.

          ### 4. Rename References
          - `vertical-discovery` skill: update "vertical" references to "pipeline" where describing the pipeline concept (not the pipeline type called "vertical")
          - `learnings-synthesis` skill: align with wrap-up stage name, update references
          - `build-session-protocol` skill: add awareness of pipeline entity concepts

          ## Validation
          - All skills can still be invoked (use Skill tool to test loading)
          - No remaining old stage slugs used as stage identifiers in skill content
          - Artifact path documentation mentions both entity-first and legacy patterns
          - Grep for old stage names returns zero results (excluding skill name references)

          ## Output
          Commit message prefix: "feat(skills): update stage references to new short slugs"

  # ── Wave 2: Orchestrator Execution Pipeline ─────────────────────────
  - name: "Orchestrator Execution Pipeline"
    serial: true
    sessions:
      - topic: work-execution-pipeline
        stage: build
        dependsOn: work-pipeline-mgmt
        prompt: |
          You are implementing Wave 2 of the Pipeline Architecture redesign (#192).
          This is the most complex session — implementing the execution commands that drive the full pipeline lifecycle.

          ## Context
          Read these docs FIRST:
          1. `docs/tools/work-orchestrator/20260215-pipeline-arch/impl-plan.md` — Wave 2 section (all tasks)
          2. `docs/research/2026-02-15-pipeline-architecture-research.md` — Phase Architecture, Stage Gates, Work Command Set, Automation Boundaries sections
          3. `scripts/work.sh` — current CLI (updated by Wave 0 + Wave 1)
          4. `scripts/lib/pipeline-entity.sh` — entity functions
          5. `scripts/lib/pipeline-registry.sh` — registry functions
          6. `scripts/lib/pipeline-define.sh` — define command (from Wave 1)
          7. `scripts/lib/pipeline-status.sh` — status command (from Wave 1)
          8. `config/pipeline-gates.json` — stage gate definitions (from Wave 0)
          9. `config/pipeline-types.json` — pipeline type definitions (from Wave 0)

          ## Your Tasks (in order)

          ### 1. Stage Gate Validation (scripts/lib/pipeline-gates.sh)
          - Load gate definitions from config/pipeline-gates.json
          - _pipeline_check_gate(): validate artifact existence, handle gate types
          - _pipeline_auto_override(): in --auto mode, human gates become artifact-exists
          - _pipeline_report_missing(): list missing artifacts blocking gate passage

          ### 2. `work start` (scripts/lib/pipeline-start.sh)
          Pre-build orchestration:
          - Validate pipeline is in ready state
          - Create ONE worktree for entire pre-build (session/MMDD-<name>-prebuild)
          - Launch Claude with pipeline context (issue, product/tool docs, prior wrap-ups, ROADMAP.md)
          - Sequentially run pre-build stages based on pipeline type's stage list
          - After each stage: validate gate, update pipeline state, record artifacts
          - Transition: ready -> active
          - After plan stage: if --auto, transition to building; else wait for human approval
          - Create prompt templates in scripts/prompts/ for each pre-build stage

          ### 3. `work build` Update (scripts/lib/pipeline-build.sh)
          Refactor existing _work_build for pipeline awareness:
          - Create base branch: build/<pipeline-id> from main
          - Read manifest from pipeline artifact directory
          - Session branches from base branch, PRs target base branch
          - Track sessions in pipeline entity
          - Auto-advance waves when all sessions merge (gh CLI polling)
          - Transition: active -> building
          - Keep backward compat: if no pipeline context, use current behavior
          - After final wave: transition to reviewing

          ### 4. `work end` (scripts/lib/pipeline-end.sh)
          Post-build orchestration:
          - Create final PR: base branch -> main
          - Run review stage (checklist from breadboard, design audit)
          - Transition: building -> reviewing
          - Merge detection polling (gh pr view, 90s intervals)
          - After merge: run wrap-up stage
          - Generate wrap-up doc: built items, plan deviations, patterns, review issues, learnings, recommended agent memory updates
          - Transition: reviewing -> wrapped

          ### 5. `work cooldown` (scripts/lib/pipeline-cooldown.sh)
          Batch processing:
          - Find all pipelines in wrapped state
          - Read all wrap-up docs since last cooldown
          - Synthesize cross-cutting themes
          - Update PROGRESS.md
          - Transition: wrapped -> cooled

          ### 6. --auto Mode + Dispatcher
          - Auto flag from pipeline entity propagated to gates and merge detection
          - Update work.sh dispatcher: add start, end, cooldown routes
          - Update build route to use pipeline-build.sh when pipeline context exists
          - Update help text

          ### 7. Integration Testing
          End-to-end smoke test:
          - work define test-exec --type bug-fix
          - work status test-exec (verify ready state)
          - Manually test stage gate functions
          - work clean test-exec (cleanup)

          ## Shell Coding Standards
          - Use local variables, avoid global state
          - jq for JSON manipulation
          - Error messages to stderr
          - Return codes: 0 success, 1 user error, 2 system error
          - _pipeline_ prefix for pipeline functions, _work_ for commands
          - Each lib file should have a header comment explaining its role

          ## Validation
          - `source scripts/work.sh && work help` — shows all new commands
          - Stage gate functions work with pipeline-gates.json
          - work start validates pipeline state correctly
          - work build creates base branch and uses it for sessions
          - work end creates PR and starts merge polling
          - work cooldown finds wrapped pipelines
          - All existing work commands still function (backward compat)

          ## Output
          Commit message prefix: "feat(work): execution pipeline — start, build, end, cooldown, gates"
